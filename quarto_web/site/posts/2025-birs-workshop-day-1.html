<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-08-19">
<meta name="description" content="Monday, August 18 Â· Day 1 of the 2025 BIRS Workshop â€œFoundation Models and Their Biomedical Applications: Bridging the Gapâ€">

<title>2025 Workshop at BIRS: Day 1 Recordings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#workshop-at-birs-overview" id="toc-workshop-at-birs-overview" class="nav-link active" data-scroll-target="#workshop-at-birs-overview">ğŸ 2025 Workshop at BIRS: Overview</a>
  <ul>
  <li><a href="#foundation-models-and-their-biomedical-applications-bridging-the-gap" id="toc-foundation-models-and-their-biomedical-applications-bridging-the-gap" class="nav-link" data-scroll-target="#foundation-models-and-their-biomedical-applications-bridging-the-gap">Foundation Models and Their Biomedical Applications: Bridging the Gap</a></li>
  <li><a href="#talks-quick-looks-full-notes-recordings" id="toc-talks-quick-looks-full-notes-recordings" class="nav-link" data-scroll-target="#talks-quick-looks-full-notes-recordings"><small><em>ğŸ¬ Talks â€” <span style="font-weight:normal;">Quick Looks, Full Notes &amp; Recordings</span></em></small></a></li>
  </ul></li>
  <li><a href="#day-1-recordings-morning-session" id="toc-day-1-recordings-morning-session" class="nav-link" data-scroll-target="#day-1-recordings-morning-session">â–¶ï¸ Day 1 Recordings: Morning Session</a>
  <ul>
  <li><a href="#hongtu-zhu-causal-generalist-medical-ai" id="toc-hongtu-zhu-causal-generalist-medical-ai" class="nav-link" data-scroll-target="#hongtu-zhu-causal-generalist-medical-ai">ğŸ¤ Hongtu Zhu: <em>Causal Generalist Medical AI</em></a></li>
  <li><a href="#shu-yang-integrating-diverse-evidence-sources-in-clinical-research-bridging-randomized-trials-and-real-world-data" id="toc-shu-yang-integrating-diverse-evidence-sources-in-clinical-research-bridging-randomized-trials-and-real-world-data" class="nav-link" data-scroll-target="#shu-yang-integrating-diverse-evidence-sources-in-clinical-research-bridging-randomized-trials-and-real-world-data">ğŸ¤ Shu Yang: <em>Integrating Diverse Evidence Sources in Clinical Research: Bridging Randomized Trials and Real-World Data</em></a></li>
  <li><a href="#jian-huang-advancing-statistical-frontiers-leveraging-large-models-in-statistical-analysis" id="toc-jian-huang-advancing-statistical-frontiers-leveraging-large-models-in-statistical-analysis" class="nav-link" data-scroll-target="#jian-huang-advancing-statistical-frontiers-leveraging-large-models-in-statistical-analysis">ğŸ¤ Jian Huang: <em>Advancing Statistical Frontiers: Leveraging Large Models in Statistical Analysis</em></a></li>
  <li><a href="#yong-chen-causal-ai-beyond-randomized-controlled-trials-negative-control-calibration-and-federated-learning" id="toc-yong-chen-causal-ai-beyond-randomized-controlled-trials-negative-control-calibration-and-federated-learning" class="nav-link" data-scroll-target="#yong-chen-causal-ai-beyond-randomized-controlled-trials-negative-control-calibration-and-federated-learning">ğŸ¤ Yong Chen: <em>Causal AI Beyond Randomized Controlled Trials: Negative Control Calibration and Federated Learning</em></a></li>
  </ul></li>
  <li><a href="#day-1-recordings-afternoon-session-1" id="toc-day-1-recordings-afternoon-session-1" class="nav-link" data-scroll-target="#day-1-recordings-afternoon-session-1">â–¶ï¸ Day 1 Recordings: Afternoon Session 1</a>
  <ul>
  <li><a href="#ross-mitchell-applications-of-foundational-models-in-medical-imaging" id="toc-ross-mitchell-applications-of-foundational-models-in-medical-imaging" class="nav-link" data-scroll-target="#ross-mitchell-applications-of-foundational-models-in-medical-imaging">ğŸ¤ Ross Mitchell: <em>Applications of Foundational Models in Medical Imaging</em></a></li>
  <li><a href="#yuehua-cui-addressing-some-challenges-in-spatial-transcriptomics-spatial-deconvolution-gene-variability-and-domain-detection" id="toc-yuehua-cui-addressing-some-challenges-in-spatial-transcriptomics-spatial-deconvolution-gene-variability-and-domain-detection" class="nav-link" data-scroll-target="#yuehua-cui-addressing-some-challenges-in-spatial-transcriptomics-spatial-deconvolution-gene-variability-and-domain-detection">ğŸ¤ Yuehua Cui: <em>Addressing some challenges in spatial transcriptomics: spatial deconvolution, gene variability, and domain detection</em></a></li>
  <li><a href="#ting-li-principal-component-analysis-in-geodesic-space" id="toc-ting-li-principal-component-analysis-in-geodesic-space" class="nav-link" data-scroll-target="#ting-li-principal-component-analysis-in-geodesic-space">ğŸ¤ Ting Li: <em>Principal Component Analysis in Geodesic Space</em></a></li>
  </ul></li>
  <li><a href="#day-1-recordings-afternoon-session-2" id="toc-day-1-recordings-afternoon-session-2" class="nav-link" data-scroll-target="#day-1-recordings-afternoon-session-2">â–¶ï¸ Day 1 Recordings: Afternoon Session 2</a>
  <ul>
  <li><a href="#peter-song-challenges-in-calculating-epigenetic-age" id="toc-peter-song-challenges-in-calculating-epigenetic-age" class="nav-link" data-scroll-target="#peter-song-challenges-in-calculating-epigenetic-age">ğŸ¤ Peter Song: <em>Challenges in Calculating Epigenetic Age</em></a></li>
  <li><a href="#ting-ye-multimodal-ai-for-predicting-atrial-fibrillation-and-heart-failure-using-ecg-and-cardiac-mri" id="toc-ting-ye-multimodal-ai-for-predicting-atrial-fibrillation-and-heart-failure-using-ecg-and-cardiac-mri" class="nav-link" data-scroll-target="#ting-ye-multimodal-ai-for-predicting-atrial-fibrillation-and-heart-failure-using-ecg-and-cardiac-mri">ğŸ¤ Ting Ye: <em>Multimodal AI for Predicting Atrial Fibrillation and Heart Failure Using ECG and Cardiac MRI</em></a></li>
  <li><a href="#zhengwu-zhang-representation-learning-and-generative-models-in-network-data-analysis" id="toc-zhengwu-zhang-representation-learning-and-generative-models-in-network-data-analysis" class="nav-link" data-scroll-target="#zhengwu-zhang-representation-learning-and-generative-models-in-network-data-analysis">ğŸ¤ Zhengwu Zhang: <em>Representation Learning and Generative Models in Network Data Analysis</em></a></li>
  <li><a href="#yumou-qiu-physics-informed-statistical-data-fusion-for-reconstructing-3d-current-fields-of-oceanic-eddies" id="toc-yumou-qiu-physics-informed-statistical-data-fusion-for-reconstructing-3d-current-fields-of-oceanic-eddies" class="nav-link" data-scroll-target="#yumou-qiu-physics-informed-statistical-data-fusion-for-reconstructing-3d-current-fields-of-oceanic-eddies">ğŸ¤ Yumou Qiu: <em>Physics-informed Statistical Data Fusion for Reconstructing 3D Current Fields of Oceanic Eddies</em></a></li>
  </ul></li>
  <li><a href="#watch-all-recordings" id="toc-watch-all-recordings" class="nav-link" data-scroll-target="#watch-all-recordings">ğŸ“Œ Watch All Recordings</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">2025 Workshop at BIRS: Day 1 Recordings</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Events</div>
    <div class="quarto-category">Videos</div>
  </div>
  </div>

<div>
  <div class="description">
    Monday, August 18 Â· Day 1 of the 2025 BIRS Workshop <strong>â€œFoundation Models and Their Biomedical Applications: Bridging the Gapâ€</strong>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<style>
  .yt-thumb-card{display:flex; align-items:center; justify-content: center; gap:12px;}
  .yt-thumb-card .avatar{width:56px; height:56px; border-radius:50%;}
  .yt-thumb-card .thumb{width:260px; aspect-ratio:16/9; object-fit:cover; border-radius:10px;}
</style>
<!--A minimal thumbnail:-->
<div style="text-align: center; justify-content: center; margin-bottom: 2em;">
<a class="yt-thumb-card" href="https://www.youtube.com/@StatsUpAI" target="_blank"> <img class="avatar" src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj" alt="Stats Up AI" style="width: 20%; height: 20%;"> </a> <a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"> <img src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" alt="Stats Up AI YouTube" width="15%">
<p style="font-size: 14px; margin-top: 0.5em;">
Visit the <strong>Stats Up AI</strong> Channel for More
</p>
</a><p><a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"></a> <!--</div>--></p>
<!--An inline frame (iframe) to embed the YouTube playlist:-->
<!--<div style="text-align: center; margin-top: 1em;">-->
<!--<iframe -->
<!--  max-width="560px"-->
<!--  width="100%"-->
<!--  aspect-ratio="16/9"-->
<!--  src="https://www.youtube.com/embed/videoseries?list=UU2VlJBtA_63CrXP0Sr99P8Q" -->
<!--  title="Uploads playlist"-->
<!--  frameborder="0"-->
<!--  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" -->
<!--  allowfullscreen>-->
<!--</iframe>-->
<!--</div>-->
</div>
<!--YouTube logo:-->
<!--src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" -->
<!--Channel avatar:-->
<!--src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj"-->
<section id="workshop-at-birs-overview" class="level2">
<h2 class="anchored" data-anchor-id="workshop-at-birs-overview">ğŸ 2025 Workshop at BIRS: Overview</h2>
<section id="foundation-models-and-their-biomedical-applications-bridging-the-gap" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models-and-their-biomedical-applications-bridging-the-gap">Foundation Models and Their Biomedical Applications: Bridging the Gap</h3>
<!--**Location:** -->
<p>ğŸ“ Banff International Research Station (BIRS), Banff, Alberta, Canada<br>
<strong>Event Website:</strong> <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470">2025 Workshop Homepage</a> Â· <strong>Dates:</strong> Aug 17â€“22, 2025</p>
</section>
<section id="talks-quick-looks-full-notes-recordings" class="level3">
<h3 class="anchored" data-anchor-id="talks-quick-looks-full-notes-recordings"><small><em>ğŸ¬ Talks â€” <span style="font-weight:normal;">Quick Looks, Full Notes &amp; Recordings</span></em></small></h3>
<div style="margin-left:20px; margin-bottom:-10px">
<small> â®• Full program: <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/schedule">2025 Workshop Schedule</a> </small>
</div>
<p><small></small></p><small>
<ul>
<li>ğŸ“– <strong>2025 Workshop at BIRS: Day 1</strong> Recordings (Monday, Aug 18) ğŸ‘ˆ<br>
</li>
<li>ğŸ”— <a href="../posts/2025-birs-workshop-day-2.html"><strong>2025 Workshop at BIRS: Day 2</strong> Recordings</a> (Tuesday, Aug 19)<br>
</li>
<li>ğŸ”— <a href="../posts/2025-birs-workshop-day-3.html"><strong>2025 Workshop at BIRS: Day 3</strong> Recordings</a> (Wednesday, Aug 20)<br>
</li>
<li>ğŸ”— <a href="../posts/2025-birs-workshop-day-4.html"><strong>2025 Workshop at BIRS: Day 4</strong> Recordings</a> (Thursday, Aug 21)</li>
</ul>
</small><p><small></small></p>
<div style="text-align:left; margin-bottom: 50px; margin-top: 20px">
<p><small style="text-align:left; border-bottom: 1px dashed #ddd;"> <em><strong>â†©ï¸</strong> Read more on <strong>Stats Up AI</strong> <a href="https://statsupai.org/community-news.html"><span style="color: #3C898A"><strong>ğŸ“° Community News</strong></span></a></em> </small></p>
</div>
</section>
</section>
<section id="day-1-recordings-morning-session" class="level2">
<h2 class="anchored" data-anchor-id="day-1-recordings-morning-session">â–¶ï¸ Day 1 Recordings: Morning Session</h2>
<section id="hongtu-zhu-causal-generalist-medical-ai" class="level3">
<h3 class="anchored" data-anchor-id="hongtu-zhu-causal-generalist-medical-ai">ğŸ¤ Hongtu Zhu: <em>Causal Generalist Medical AI</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 09:05 â€“ 09:45<br>
ğŸ›ï¸ The University of North Carolina at Chapel Hill</p>
<strong>Keywords:</strong> Causal inference, generalist AI, medical decision-making, interpretability, robustness, generalizability, multimodal datasets, causal discovery, counterfactual reasoning, domain adaptation, clinical reliability<br>
<strong>What It Does:</strong> Introducing Causal Generalist Medical AI, which integrates causal reasoning with generalist AI models to improve interpretability, robustness, and generalizability in medical decision-making.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> The rapid evolution of flexible and reusable artificial intelligence (AI) models is transforming medical science. We will introduce Causal Generalist Medical AI (Causal GMAI)â€”a paradigm that integrates causal inference with generalist AI models to enhance interpretability, robustness, and generalizability in medical decision-making. Causal GMAI employs self-supervised, semi-supervised, and supervised learning on diverse multimodal datasetsâ€”including imaging, electronic health records, clinical trials, laboratory results, genomics, knowledge graphs, and medical textâ€”to perform a wide range of tasks with minimal task-specific supervision. By embedding causal reasoning, these models go beyond prediction to infer underlying causal relationships, improving diagnostic accuracy, treatment recommendations, and personalized medicine. The course covers key technical components such as causal discovery, counterfactual reasoning, and domain adaptation, alongside real-world applications. We will also explore challenges in regulation, validation, and dataset curation to ensure clinical reliability and ethical deployment. Designed for researchers, clinicians, data scientists, and AI practitioners, this course provides a foundation for advancing the next generation of trustworthy and interpretable medical AI.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508180905-Zhu.mp4" title="BIRS talk: Hongtu Zhu" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508180905-Zhu.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="shu-yang-integrating-diverse-evidence-sources-in-clinical-research-bridging-randomized-trials-and-real-world-data" class="level3">
<h3 class="anchored" data-anchor-id="shu-yang-integrating-diverse-evidence-sources-in-clinical-research-bridging-randomized-trials-and-real-world-data">ğŸ¤ Shu Yang: <em>Integrating Diverse Evidence Sources in Clinical Research: Bridging Randomized Trials and Real-World Data</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 09:52 â€“ 10:24<br>
ğŸ›ï¸ North Carolina State University</p>
<strong>Keywords:</strong> Clinical trials, real-world data, precision medicine, evidence integration, statistical framework, 21st Century Cures Act, FDA, causal inference, bias assessment, hybrid trial designs, conformal prediction<br>
<strong>What It Does:</strong> Presenting a systematic framework for integrating randomized clinical trials and real-world data to enhance precision medicine development.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> The 21st Century Cures Act, enacted in 2016, empowers the U.S. Food and Drug Administration to accelerate the development and evaluation of new medical treatments by leveraging real-world data (RWD) and real-world evidence. With the increasing availability of both randomized clinical trials (RCTs) and RWD, integrating these heterogeneous sources of evidence offers unique opportunities to address clinical questions that neither can answer in isolation. This talk presents a systematic framework for combining evidence from RCTs and real-world studies, with a focus on enhancing precision medicine development. The presentation will cover the following key topics: 1) The Evolving Landscape of RWD in Clinical Research: An overview of how RWD is used across different stages of the clinical development lifecycle and study designs. 2) Key Objectives for Integrative Evidence Synthesis: how integration of RCT and RWD can improve the generalizability and transportability of RCT findings, increase the efficiency and statistical power of treatment effect estimation, and enable long-term safety and effectiveness monitoring. 3) A Causal Roadmap for Evidence Integration: a causal inference perspective to articulate the assumptions, identification strategies, and inferential goals when combining RCTs and RWD. Emphasis is placed on the importance of understanding and addressing bias, especially in regulatory contexts. 4) The Role of AI/ML and Statistical Rigor: While artificial intelligence and machine learning offer powerful tools for data integration and prediction, rigorous statistical thinking remains paramount for valid causal inference and bias mitigation. 5) Innovative Trial Designs: recent advances in hybrid controlled trials using external real-world controls, such as test-then-pool procedures, selective borrowing, and conformal prediction approaches, with a focus on improving efficiency while safeguarding validity. 6) Challenges and Opportunities: conclusion by examining unresolved challenges and outlining future research directions.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508180952-Yang.mp4" title="BIRS talk: Shu Yang" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508180952-Yang.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="jian-huang-advancing-statistical-frontiers-leveraging-large-models-in-statistical-analysis" class="level3">
<h3 class="anchored" data-anchor-id="jian-huang-advancing-statistical-frontiers-leveraging-large-models-in-statistical-analysis">ğŸ¤ Jian Huang: <em>Advancing Statistical Frontiers: Leveraging Large Models in Statistical Analysis</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 10:44 â€“ 11:20<br>
ğŸ›ï¸ The Hong Kong Polytechnic University</p>
<strong>Keywords:</strong> Large-scale machine learning, foundation models, statistical challenges, contemporary methods, conditional generative learning, functional protein sequence generation, synthetic data augmentation, code-free data analysis<br>
<strong>What It Does:</strong> Examining how large-scale machine learning models, including foundation models, can address contemporary statistical challenges with illustrative examples.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> The advent of large-scale machine learning models, including deep neural networks and foundation models, is fundamentally reshaping the field of statistics. In this talk, we will examine how these powerful models can be leveraged to tackle contemporary statistical challenges. Through illustrative examples, including conditional generative learning, functional protein sequence generation, synthetic data augmentation, and code-free data analysis, we will highlight both the opportunities and the challenges that large models introduce to statistics.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181044-Huang.mp4" title="BIRS talk: Jian Huang" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181044-Huang.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="yong-chen-causal-ai-beyond-randomized-controlled-trials-negative-control-calibration-and-federated-learning" class="level3">
<h3 class="anchored" data-anchor-id="yong-chen-causal-ai-beyond-randomized-controlled-trials-negative-control-calibration-and-federated-learning">ğŸ¤ Yong Chen: <em>Causal AI Beyond Randomized Controlled Trials: Negative Control Calibration and Federated Learning</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 11:23 â€“ 11:55<br>
ğŸ›ï¸ University of Pennsylvania</p>
<strong>Keywords:</strong> Causal inference, negative control calibration, federated learning, randomized controlled trials, unmeasured confounding, NCOs, privacy-preserving, vaccine safety surveillance, drug repurposing, oncology research<br>
<strong>What It Does:</strong> Proposing a framework using negative control calibration and federated learning to strengthen causal inference beyond randomized controlled trials.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> Causal inference from real-world data (RWD) is often threatened by unmeasured confoundingâ€”biases that remain even after adjusting for observed covariates. To address this fundamental challenge, we introduce a framework leveraging negative control outcomes (NCOs) to diagnose and correct for hidden biases. Our approach improves the validity of causal effect estimates across a wide range of adjustment strategies, offering a practical and generalizable path toward trustworthy inference.</p>
<p>However, single-site analyses are often limited by insufficient sample sizes and lack of generalizability, especially when studying rare outcomes. To overcome this barrier, we further integrate federated learning, enabling multi-site collaboration without sharing patient-level data. This allows us to scale NCO-based calibration across institutions, while also supporting privacy-preserving subphenotyping and target trial emulation.</p>
<p>Together, these tools form the backbone of a Causal AI framework that is both debiased and distributedâ€”capable of delivering robust, generalizable insights across fragmented health data ecosystems. Applications include vaccine safety surveillance, drug repurposing, and international oncology research, demonstrating the real-world impact of combining statistical rigor with collaborative infrastructure.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181123-Chen.mp4" title="BIRS talk: Yong Chen" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181123-Chen.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
</section>
<section id="day-1-recordings-afternoon-session-1" class="level2">
<h2 class="anchored" data-anchor-id="day-1-recordings-afternoon-session-1">â–¶ï¸ Day 1 Recordings: Afternoon Session 1</h2>
<section id="ross-mitchell-applications-of-foundational-models-in-medical-imaging" class="level3">
<h3 class="anchored" data-anchor-id="ross-mitchell-applications-of-foundational-models-in-medical-imaging">ğŸ¤ Ross Mitchell: <em>Applications of Foundational Models in Medical Imaging</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 13:33 â€“ 14:05<br>
ğŸ›ï¸ University of Alberta</p>
<strong>Keywords:</strong> Foundational models, medical imaging, classification, segmentation, uncertainty modeling, Microsoft MII, self-supervised learning, multi-modal training, probabilistic 3D U-Net, organs-at-risk segmentation, cone-beam CT<br>
<strong>What It Does:</strong> Demonstrating applications of foundational models in medical imaging for classification, segmentation, and uncertainty modeling across large-scale datasets.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> This presentation explores the application of foundational models to medical imaging challenges, addressing both classification and segmentation tasks in label-limited scenarios. The first portion provides an overview of current foundational models in medical imaging, including Microsoftâ€™s Medical Imaging Intelligence (MII) model and similar architectures that have transformed the field through large-scale pre-training on diverse medical datasets. We examine how these models leverage self-supervised learning and multi-modal training to achieve remarkable performance across various medical imaging tasks, establishing new benchmarks for diagnostic accuracy and clinical utility.</p>
<p>The second portion presents our probabilistic approach to medical image segmentation using 40,000 3D CT scans from 10,000 patients for intestinal tract segmentation. We employ multiple foundational image segmentation models to train a probabilistic 3D U-Net that explicitly models uncertainty in ground truth annotations, learning per-voxel probability distributions across segmentation tasks. We demonstrate how this probabilistic framework extends naturally to challenging applications where ground truth labels are unavailable or prohibitively expensive, specifically applying our method to organs-at-risk segmentation in cone-beam CT scans for head and neck cancer radiotherapy planning.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181333-Mitchell.mp4" title="BIRS talk: Ross Mitchell" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181333-Mitchell.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="yuehua-cui-addressing-some-challenges-in-spatial-transcriptomics-spatial-deconvolution-gene-variability-and-domain-detection" class="level3">
<h3 class="anchored" data-anchor-id="yuehua-cui-addressing-some-challenges-in-spatial-transcriptomics-spatial-deconvolution-gene-variability-and-domain-detection">ğŸ¤ Yuehua Cui: <em>Addressing some challenges in spatial transcriptomics: spatial deconvolution, gene variability, and domain detection</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 14:06 â€“ 14:26<br>
ğŸ›ï¸ Michigan State University</p>
<strong>Keywords:</strong> Spatial transcriptomics, spatial deconvolution, gene variability, domain detection, statistical methods, 10x Visium, spatially variable genes, cell type-specific SVGs, linear mixed-effect model, tumor progression, deep learning<br>
<strong>What It Does:</strong> Developing statistical and deep learning methods to address challenges in spatial transcriptomics, including deconvolution, gene variability, and domain detection.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> Spatial transcriptomics (ST) provides crucial insights into tissue-specific gene expression patterns in various studies. In this talk, I will focus on three major tasks in ST data analysis: spatial deconvolution, spatial and temporal gene identification, and spatial domain detection. For spatial deconvolution, I will present a reference-free deconvolution method for spot-level ST data, such as those obtained from the 10x Visium platform, to infer cell type compositions in each spot. While recent methodological developments have greatly advanced the detection of spatially variable genes (SVGs), whose expression patterns are non-random across tissue locations, such SVGs do not reveal cellular heterogeneity in a spatial context. Following spatial deconvolution, I will introduce a unified approach to identify both SVGs and cell type-specific SVGs (ctSVGs), under a linear mixed-effect model framework. I will further show how we can incorporate cell trajectory information to identify genes showing spatial and temporal variation, offering critical insight into tumor progression and dynamics. Finally, I will present a deep learning framework for improved spatial domain detection.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181406-Cui.mp4" title="BIRS talk: Yuehua Cui" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181406-Cui.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="ting-li-principal-component-analysis-in-geodesic-space" class="level3">
<h3 class="anchored" data-anchor-id="ting-li-principal-component-analysis-in-geodesic-space">ğŸ¤ Ting Li: <em>Principal Component Analysis in Geodesic Space</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 14:27 â€“ 14:57<br>
ğŸ›ï¸ Hong Kong Polytechnic University</p>
<strong>Keywords:</strong> Principal component analysis, geodesic space, complex data structures, neuroimaging, Geodesic-PCA, G-PCA, brain corpus callosum, task-fMRI, Riemannian manifolds<br>
<strong>What It Does:</strong> Extending principal component analysis into geodesic spaces to analyze complex data structures such as neuroimaging.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> Principal Component Analysis (PCA) has been widely applied and extensively studied. However, it presents significant challenges to extend it for complex data in metric spaces. In this work, we propose Geodesic-PCA (G-PCA), a unified framework that extends PCA to geodesic spaces beyond manifolds. We develop robust and optimal theoretical results for G-PCA and validate its reliability and effectiveness through extensive simulations. In practical applications, we apply G-PCA to analyze brain corpus callosum and task-fMRI data, demonstrating its potential in fields such as neuroimaging.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181427-Li.mp4" title="BIRS talk: Ting Li" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181427-Li.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
</section>
<section id="day-1-recordings-afternoon-session-2" class="level2">
<h2 class="anchored" data-anchor-id="day-1-recordings-afternoon-session-2">â–¶ï¸ Day 1 Recordings: Afternoon Session 2</h2>
<section id="peter-song-challenges-in-calculating-epigenetic-age" class="level3">
<h3 class="anchored" data-anchor-id="peter-song-challenges-in-calculating-epigenetic-age">ğŸ¤ Peter Song: <em>Challenges in Calculating Epigenetic Age</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 15:31 â€“ 16:00<br>
ğŸ›ï¸ University of Michigan</p>
<strong>Keywords:</strong> Epigenetic age, predictive models, high-resolution data, uncertainty quantification, multiple clocks, DNA methylation, epigenetic clocks, convolutional neural networks, conformal prediction, aging research<br>
<strong>What It Does:</strong> Addressing challenges in calculating epigenetic age by refining predictive models with high-resolution data, uncertainty quantification, and multiple clocks.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> DNA methylation (DNAm) has emerged as a key source of omics data for assessing epigenetic age, offering a wealth of genetic markers that reflect cellular changes influenced by social and environmental factors. Epigenetic age can be estimated through predictive models known as epigenetic clocks, which rely on high-dimensional data analytics. However, current epigenetic age calculators face significant limitations as DNAm data collection technology rapidly advances. In this talk, I will present approaches to tackle a few data science challenges, including refining epigenetic clocks with higher-resolution DNAm data using convolutional neural networks, quantifying prediction uncertainty using conformal prediction techniques to address increasing variability over aging, and combining multiple epigenetic clocks. This presentation will integrate both computational methodologies and algorithmic solutions, demonstrated through real-world data applications.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181531-Song.mp4" title="BIRS talk: Peter Song" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181531-Song.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="ting-ye-multimodal-ai-for-predicting-atrial-fibrillation-and-heart-failure-using-ecg-and-cardiac-mri" class="level3">
<h3 class="anchored" data-anchor-id="ting-ye-multimodal-ai-for-predicting-atrial-fibrillation-and-heart-failure-using-ecg-and-cardiac-mri">ğŸ¤ Ting Ye: <em>Multimodal AI for Predicting Atrial Fibrillation and Heart Failure Using ECG and Cardiac MRI</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 16:07 â€“ 16:28<br>
ğŸ›ï¸ University of Washington</p>
<strong>Keywords:</strong> Multimodal AI, atrial fibrillation, heart failure, ECG, cardiac MRI, deep learning, cardiovascular risk stratification, UK Biobank, shared representations, modality-specific representations, disease prevention<br>
<strong>What It Does:</strong> Introducing a multimodal deep learning framework that integrates ECG and cardiac MRI to improve prediction of atrial fibrillation and heart failure.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> Atrial fibrillation (AF) and heart failure (HF) are leading causes of cardiovascular morbidity, mortality, and healthcare burden worldwide. Early detection of individuals at elevated risk, especially those who are asymptomatic, is critical for timely intervention and disease prevention. In this talk, I will present a novel multimodal deep learning framework that integrates electrocardiogram (ECG) and cardiac magnetic resonance imaging (MRI) data to jointly learn shared and modality-specific representations. By combining the temporal features of ECG with the structural insights from Cardiac MRI, the model significantly improves predictive performance on key clinical tasks, including predicting the onset of AF and HF. The framework is developed and evaluated using data from the UK Biobank, demonstrating the potential of multimodal AI to enhance cardiovascular risk stratification and inform targeted prevention strategies.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181607-Ye.mp4" title="BIRS talk: Ting Ye" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181607-Ye.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="zhengwu-zhang-representation-learning-and-generative-models-in-network-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="zhengwu-zhang-representation-learning-and-generative-models-in-network-data-analysis">ğŸ¤ Zhengwu Zhang: <em>Representation Learning and Generative Models in Network Data Analysis</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 16:29 â€“ 16:50<br>
ğŸ›ï¸ The University of North Carolina at Chapel Hill</p>
<strong>Keywords:</strong> Representation learning, generative models, VAEs, brain networks, neuroimaging, Variational Auto-Encoders, GATE, motion-invariant VAE, inv-VAE, brain connectivity, cognitive trait prediction<br>
<strong>What It Does:</strong> Demonstrating how representation learning with VAEs and generative models can extract meaningful embeddings from brain networks to advance neuroimaging.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> Brain network analysis grapples with high-dimensional, complex data. This talk will focus on how representation learning, particularly through Variational Auto-Encoders (VAEs), offers a powerful framework to extract meaningful low-dimensional embeddings from brain networks. We will delve into how VAEs learn latent representations that not only allow for accurate reconstruction of the original network data but also serve as a versatile foundation for diverse downstream tasks, such as predicting human traits or disentangling nuisance factors. We will showcase some generative models, e.g., Graph Auto-Encoding (GATE), which characterizes brain graph population distributions to improve cognitive trait prediction and a motion-invariant VAE (inv-VAE) that learns representations robust to motion artifacts in structural connectomes. Ultimately, this talk will demonstrate the transformative potential of generation-driven representation learning for advancing neuroimaging analyses and deepening our understanding of brain structure and function.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181629-Zhang.mp4" title="BIRS talk: Zhengwu Zhang" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181629-Zhang.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
<section id="yumou-qiu-physics-informed-statistical-data-fusion-for-reconstructing-3d-current-fields-of-oceanic-eddies" class="level3">
<h3 class="anchored" data-anchor-id="yumou-qiu-physics-informed-statistical-data-fusion-for-reconstructing-3d-current-fields-of-oceanic-eddies">ğŸ¤ Yumou Qiu: <em>Physics-informed Statistical Data Fusion for Reconstructing 3D Current Fields of Oceanic Eddies</em></h3>
<p>ğŸ“… Monday, August 18, 2025 â€¢ ğŸ•˜ 16:53 â€“ 17:15<br>
ğŸ›ï¸ Peking University</p>
<strong>Keywords:</strong> Physics-informed transfer learning, multi-source ocean data, 3D current fields, real-time field campaigns, geostrophic balance, Navier-Stokes equations, GLORYS reanalysis, Kuroshio Extension, underwater gliders, ADCP observations<br>
<strong>What It Does:</strong> Applying a physics-informed transfer learning framework to integrate multi-source ocean data for reconstructing 3D current fields and guiding real-time field campaigns.
<details>
<summary>
<u>ğŸ“– Read more</u>
</summary>
<p><strong>Introduction:</strong> Accurate reconstruction of three-dimensional ocean current fields is critical for understanding ocean dynamics and real-time control of modern oceanographic field campaigns, particularly in mesoscale eddy environments. We propose a physics-informed transfer learning framework, designed for multi-source data fusion, to estimate the three-dimensional current structure of oceanic eddies by integrating satellite altimetry and temperature data, ocean reanalysis data, and in situ drifting buoy observations. The approach leverages geostrophic balance, derived from the Navier-Stokes equations, to guide a neural network trained on GLORYS reanalysis data in inferring subsurface currents from surface conditions. The surface conditions were estimated using a high-dimensional linear mixed model, which integrates systematically biased satellite altimetry and sparse drifting buoy data, allowing for spatially adaptive bias correction and yielding more accurate and spatially coherent surface velocity fields. This framework was deployed during a September 2024 field campaign targeting a cyclonic eddy in the Kuroshio Extension, guiding the real-time control of seven underwater gliders. Compared with existing data products, our method reduced RMSE by over 40% in cross-validation with drifting buoys and showed improved consistency with ADCP observations. The resulting glider trajectories provided enhanced spatial coverage of the eddy interior, enabling the first fine-scale three-dimensional survey of an eddy in the Kuroshio Extension region.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
<p><iframe src="https://videos.birs.ca/2025/25w5470/202508181653-Qiu.mp4" title="BIRS talk: Yumou Qiu" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;"> </iframe></p>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508181653-Qiu.html">ğŸ¬Open the video directly</a><br>
</p>
</section>
</section>
<section id="watch-all-recordings" class="level2">
<h2 class="anchored" data-anchor-id="watch-all-recordings">ğŸ“Œ Watch All Recordings</h2>
<ul>
<li><strong>StatsUpAI YouTube Channel:</strong> <a href="https://www.youtube.com/@StatsUpAI">Subscribe for updates</a></li>
<li><strong>BIRS Official Videos Page:</strong> <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos">2025 Workshop Videos</a></li>
<li><strong>Direct Video Downloads:</strong> <a href="https://videos.birs.ca/2025/25w5470/">BIRS Video Server</a></li>
</ul>
<!------->
<style>
  .yt-thumb-card{display:flex; align-items:center; justify-content: center; gap:12px;}
  .yt-thumb-card .avatar{width:56px; height:56px; border-radius:50%;}
  .yt-thumb-card .thumb{width:260px; aspect-ratio:16/9; object-fit:cover; border-radius:10px;}
</style>
<!--A minimal thumbnail:-->
<div style="text-align: center; justify-content: center; margin-bottom: 2em;">
<a class="yt-thumb-card" href="https://www.youtube.com/@StatsUpAI" target="_blank"> <img class="avatar" src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj" alt="Stats Up AI" style="width: 20%; height: 20%;"> </a> <a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"> <img src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" alt="Stats Up AI YouTube" width="15%">
<p style="font-size: 14px; margin-top: 0.5em;">
Visit the <strong>Stats Up AI</strong> Channel for More
</p>
</a><p><a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"></a> <!--</div>--></p>
<!--An inline frame (iframe) to embed the YouTube playlist:-->
<!--<div style="text-align: center; margin-top: 1em;">-->
<!--<iframe -->
<!--  max-width="560px"-->
<!--  width="100%"-->
<!--  aspect-ratio="16/9"-->
<!--  src="https://www.youtube.com/embed/videoseries?list=UU2VlJBtA_63CrXP0Sr99P8Q" -->
<!--  title="Uploads playlist"-->
<!--  frameborder="0"-->
<!--  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" -->
<!--  allowfullscreen>-->
<!--</iframe>-->
<!--</div>-->
</div>
<!--YouTube logo:-->
<!--src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" -->
<!--Channel avatar:-->
<!--src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj"-->
<p><em>AI is rapidly reshaping biomedical research by integrating diverse data, accelerating discovery, and supporting decision-making under uncertainty. With statisticians at the forefront, these applications gain the depth, rigor, and reliability needed to truly transform science and medicine.</em></p>


</section>

</main> <!-- /main -->
<!-- GoatCounter Tracker -->
<script data-goatcounter="https://statsupai.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

<!-- Per-Page View Counter (Footer Style) -->
<style>
#goatcounter-footer {
  --gc-counter-color: #2b786bef;
  display: flex; 
  align-items: center; 
  column-gap: 5px;
  color: #8e8e8e; 
  font-size: 13px; 
  font-family: Arial, sans-serif;
}
#goatcounter-footer .statcounter {
  font-family: 'Courier New', monospace; 
  color: var(--gc-counter-color);
  font-weight: bold; 
  margin-left: 3px; 
}
</style>

<!-- GoatCounter Tracker -->
<div id="goatcounter-footer"><span style="margin-left: 3px;">Loading views...</span></div>
<script>
    const path = window.location.pathname;
    console.log('window.location.pathname:', path);
    // If at the root, skip fetching views for now
    if (path.endsWith('index.html') || path.endsWith('/posts/')) {
      
      const display = document.getElementById("goatcounter-footer");
      if (display) {
        display.innerHTML = "";
        console.log("Skipping views for root path:", path);
      }
      
    } else {
      
      // Trim path to remove everything before /quarto_web/site/ for easier local testing
      var trimmedPath = path.replace(/^.*\/posts\//, '\/quarto_web\/site\/posts\/');
      // If the path is exactly /posts/, set trimmedPath to the index.html file
      if (path === '\/posts\/') {
          console.log('Path is exactly /posts/, setting trimmedPath to index.html');
          trimmedPath = '/quarto_web/site/posts/index.html';
      }
      console.log('trimmedPath:', trimmedPath);
      
      // setInterval to check every 100ms because GoatCounter may not be available immediately
      var t = setInterval(function() {
          console.log('Checking for GoatCounter availability...window.goatcounter:', window.goatcounter);
          // Check if GoatCounter is available
          if (window.goatcounter && window.goatcounter.visit_count) {

              // GoatCounter is available, clear the timer
              clearInterval(t)
              // Resolve the counter color to a concrete value
              var footer = document.getElementById('goatcounter-footer');
              var resolvedColor = footer ? getComputedStyle(footer).getPropertyValue('--gc-counter-color').trim() : '';
              console.log('Resolved counter color:', resolvedColor);
              // Update the display with the current visit count
              const display = document.getElementById("goatcounter-footer");
              if (display) {
                display.innerHTML = '<span">Page Views: </span>';
                console.log('display.innerHTML:', display.innerHTML);
              }
              
              
              goatcounter.visit_count({
                no_branding: true,
                path: trimmedPath,
                append: '#goatcounter-footer', 
                style: `
                  body {
                    background: transparent !important;
                  }
                  div { 
                    background: transparent !important;
                    border-width: 0px;
                    border: 0 !important;
                    display: flex;
                  }
                  #gcvc-for {
                    font-size: 0px;
                  }
                  #gcvc-views { 
                      font-family: 'Courier New', monospace; 
                      font-size: 13px;
                      color: ${resolvedColor}; 
                      font-weight: bold; 
                      margin-top: 29px;
                  }`
              })

          }
      }, 100)
      
    }
</script>






<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>