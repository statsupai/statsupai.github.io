<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-08-20">
<meta name="description" content="Tuesday, August 19 Â· Day 2 of the 2025 BIRS Workshop â€œFoundation Models and Their Biomedical Applications: Bridging the Gapâ€">

<title>2025 Workshop at BIRS: Day 2 Recordings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#workshop-at-birs-overview" id="toc-workshop-at-birs-overview" class="nav-link active" data-scroll-target="#workshop-at-birs-overview">ğŸ 2025 Workshop at BIRS: Overview</a>
  <ul>
  <li><a href="#foundation-models-and-their-biomedical-applications-bridging-the-gap" id="toc-foundation-models-and-their-biomedical-applications-bridging-the-gap" class="nav-link" data-scroll-target="#foundation-models-and-their-biomedical-applications-bridging-the-gap">Foundation Models and Their Biomedical Applications: Bridging the Gap</a></li>
  <li><a href="#talks-quick-looks-full-notes-recordings" id="toc-talks-quick-looks-full-notes-recordings" class="nav-link" data-scroll-target="#talks-quick-looks-full-notes-recordings"><small><em>ğŸ¬ Talks â€” <span style="font-weight:normal;">Quick Looks, Full Notes &amp; Recordings</span></em></small></a></li>
  </ul></li>
  <li><a href="#day-2-recordings-morning-session" id="toc-day-2-recordings-morning-session" class="nav-link" data-scroll-target="#day-2-recordings-morning-session">â–¶ï¸ Day 2 Recordings: Morning Session</a>
  <ul>
  <li><a href="#talk-tianxi-cai-unlocking-the-potential-of-ehr-data-for-discovery-opportunities-and-challenges" id="toc-talk-tianxi-cai-unlocking-the-potential-of-ehr-data-for-discovery-opportunities-and-challenges" class="nav-link" data-scroll-target="#talk-tianxi-cai-unlocking-the-potential-of-ehr-data-for-discovery-opportunities-and-challenges">ğŸ¤ Tianxi Cai: <em>Unlocking the Potential of EHR Data for Discovery: Opportunities and Challenges</em></a></li>
  <li><a href="#talk-xihong-lin-navigate-the-crossroad-of-statistics-generative-ai-and-genomic-health" id="toc-talk-xihong-lin-navigate-the-crossroad-of-statistics-generative-ai-and-genomic-health" class="nav-link" data-scroll-target="#talk-xihong-lin-navigate-the-crossroad-of-statistics-generative-ai-and-genomic-health">ğŸ¤ Xihong Lin: <em>Navigate the Crossroad of Statistics, Generative AI and Genomic Health</em></a></li>
  <li><a href="#talk-annie-qu-representation-retrieval-learning-for-heterogeneous-data-integration" id="toc-talk-annie-qu-representation-retrieval-learning-for-heterogeneous-data-integration" class="nav-link" data-scroll-target="#talk-annie-qu-representation-retrieval-learning-for-heterogeneous-data-integration">ğŸ¤ Annie Qu: <em>Representation Retrieval Learning for Heterogeneous Data Integration</em></a></li>
  </ul></li>
  <li><a href="#day-2-recordings-afternoon-session-1" id="toc-day-2-recordings-afternoon-session-1" class="nav-link" data-scroll-target="#day-2-recordings-afternoon-session-1">â–¶ï¸ Day 2 Recordings: Afternoon Session 1</a>
  <ul>
  <li><a href="#talk-hongzhe-li-decoding-population-diversity-at-single-cell-resolution-with-ai-and-machine-learning" id="toc-talk-hongzhe-li-decoding-population-diversity-at-single-cell-resolution-with-ai-and-machine-learning" class="nav-link" data-scroll-target="#talk-hongzhe-li-decoding-population-diversity-at-single-cell-resolution-with-ai-and-machine-learning">ğŸ¤ Hongzhe Li: <em>Decoding Population Diversity at Single-Cell Resolution with AI and Machine Learning</em></a></li>
  <li><a href="#talk-tianming-liu-quantum-transformers" id="toc-talk-tianming-liu-quantum-transformers" class="nav-link" data-scroll-target="#talk-tianming-liu-quantum-transformers">ğŸ¤ Tianming Liu: <em>Quantum Transformers</em></a></li>
  <li><a href="#talk-tao-wang-profiling-antigen-binding-affinity-of-b-cell-repertoires-in-tumors-by-deep-learning" id="toc-talk-tao-wang-profiling-antigen-binding-affinity-of-b-cell-repertoires-in-tumors-by-deep-learning" class="nav-link" data-scroll-target="#talk-tao-wang-profiling-antigen-binding-affinity-of-b-cell-repertoires-in-tumors-by-deep-learning">ğŸ¤ Tao Wang: <em>Profiling antigen-binding affinity of B cell repertoires in tumors by deep learning</em></a></li>
  <li><a href="#talk-rong-ma-two-recent-methods-for-nonlinear-joint-embedding-of-high-dimensional-data" id="toc-talk-rong-ma-two-recent-methods-for-nonlinear-joint-embedding-of-high-dimensional-data" class="nav-link" data-scroll-target="#talk-rong-ma-two-recent-methods-for-nonlinear-joint-embedding-of-high-dimensional-data">ğŸ¤ Rong Ma: <em>Two recent methods for nonlinear joint embedding of high-dimensional data</em></a></li>
  <li><a href="#talk-yang-ning-optimal-variable-clustering-for-high-dimensional-matrix-valued-data" id="toc-talk-yang-ning-optimal-variable-clustering-for-high-dimensional-matrix-valued-data" class="nav-link" data-scroll-target="#talk-yang-ning-optimal-variable-clustering-for-high-dimensional-matrix-valued-data">ğŸ¤ Yang Ning: <em>Optimal Variable Clustering for High-Dimensional Matrix Valued Data</em></a></li>
  </ul></li>
  <li><a href="#day-2-recordings-afternoon-session-2" id="toc-day-2-recordings-afternoon-session-2" class="nav-link" data-scroll-target="#day-2-recordings-afternoon-session-2">â–¶ï¸ Day 2 Recordings: Afternoon Session 2</a>
  <ul>
  <li><a href="#talk-yingqi-zhao-early-classification-of-time-series-with-constraint" id="toc-talk-yingqi-zhao-early-classification-of-time-series-with-constraint" class="nav-link" data-scroll-target="#talk-yingqi-zhao-early-classification-of-time-series-with-constraint">ğŸ¤ Yingqi Zhao: <em>Early classification of time series with constraint</em></a></li>
  <li><a href="#talk-xiaowu-dai-training-free-multi-agent-language-models" id="toc-talk-xiaowu-dai-training-free-multi-agent-language-models" class="nav-link" data-scroll-target="#talk-xiaowu-dai-training-free-multi-agent-language-models">ğŸ¤ Xiaowu Dai: <em>Training-Free Multi-Agent Language Models</em></a></li>
  </ul></li>
  <li><a href="#watch-all-recordings" id="toc-watch-all-recordings" class="nav-link" data-scroll-target="#watch-all-recordings">ğŸ“Œ Watch All Recordings</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">2025 Workshop at BIRS: Day 2 Recordings</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Events</div>
    <div class="quarto-category">Videos</div>
  </div>
  </div>

<div>
  <div class="description">
    Tuesday, August 19 Â· Day 2 of the 2025 BIRS Workshop <strong>â€œFoundation Models and Their Biomedical Applications: Bridging the Gapâ€</strong>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 20, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<style>
  .yt-thumb-card{display:flex; align-items:center; justify-content: center; gap:12px;}
  .yt-thumb-card .avatar{width:56px; height:56px; border-radius:50%;}
  .yt-thumb-card .thumb{width:260px; aspect-ratio:16/9; object-fit:cover; border-radius:10px;}
</style>
<!--A minimal thumbnail:-->
<div style="text-align: center; justify-content: center; margin-bottom: 2em;">
<a class="yt-thumb-card" href="https://www.youtube.com/@StatsUpAI" target="_blank"> <img class="avatar" src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj" alt="Stats Up AI" style="width: 20%; height: 20%;"> </a> <a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"> <img src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" alt="Stats Up AI YouTube" width="15%">
<p style="font-size: 14px; margin-top: 0.5em;">
Visit the <strong>Stats Up AI</strong> Channel for More
</p>
</a><p><a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"></a> <!--</div>--></p>
<!--An inline frame (iframe) to embed the YouTube playlist:-->
<!--<div style="text-align: center; margin-top: 1em;">-->
<!--<iframe -->
<!--  max-width="560px"-->
<!--  width="100%"-->
<!--  aspect-ratio="16/9"-->
<!--  src="https://www.youtube.com/embed/videoseries?list=UU2VlJBtA_63CrXP0Sr99P8Q" -->
<!--  title="Uploads playlist"-->
<!--  frameborder="0"-->
<!--  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" -->
<!--  allowfullscreen>-->
<!--</iframe>-->
<!--</div>-->
</div>
<!--YouTube logo:-->
<!--src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" -->
<!--Channel avatar:-->
<!--src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj"-->
<section id="workshop-at-birs-overview" class="level2">
<h2 class="anchored" data-anchor-id="workshop-at-birs-overview">ğŸ 2025 Workshop at BIRS: Overview</h2>
<section id="foundation-models-and-their-biomedical-applications-bridging-the-gap" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models-and-their-biomedical-applications-bridging-the-gap">Foundation Models and Their Biomedical Applications: Bridging the Gap</h3>
<!--**Location:** -->
<p>ğŸ“ Banff International Research Station (BIRS), Banff, Alberta, Canada<br>
<strong>Event Website:</strong> <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470">2025 Workshop Homepage</a> Â· <strong>Dates:</strong> Aug 17â€“22, 2025</p>
</section>
<section id="talks-quick-looks-full-notes-recordings" class="level3">
<h3 class="anchored" data-anchor-id="talks-quick-looks-full-notes-recordings"><small><em>ğŸ¬ Talks â€” <span style="font-weight:normal;">Quick Looks, Full Notes &amp; Recordings</span></em></small></h3>
<div style="margin-left:20px; margin-bottom:-10px">
<small> â®• Full program: <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/schedule">2025 Workshop Schedule</a> </small>
</div>
<p><small></small></p><small>
<ul>
<li>ğŸ”— <a href="../posts/2025-birs-workshop-day-1.html"><strong>2025 Workshop at BIRS: Day 1</strong> Recordings</a> (Monday, Aug 18)<br>
</li>
<li>ğŸ“– <strong>2025 Workshop at BIRS: Day 2</strong> Recordings (Tuesday, Aug 19) ğŸ‘ˆ<br>
</li>
<li>ğŸ”— <a href="../posts/2025-birs-workshop-day-3.html"><strong>2025 Workshop at BIRS: Day 3</strong> Recordings</a> (Wednesday, Aug 20)<br>
</li>
<li>ğŸ”— <a href="../posts/2025-birs-workshop-day-4.html"><strong>2025 Workshop at BIRS: Day 4</strong> Recordings</a> (Thursday, Aug 21)</li>
</ul>
</small><p><small></small></p>
<div style="text-align:left; margin-bottom: 50px; margin-top: 20px">
<p><small style="text-align:left; border-bottom: 1px dashed #ddd;"> <em><strong>â†©ï¸</strong> Read more on <strong>Stats Up AI</strong> <a href="https://statsupai.org/community-news.html"><span style="color: #3C898A"><strong>ğŸ“° Community News</strong></span></a></em> </small></p>
</div>
</section>
</section>
<section id="day-2-recordings-morning-session" class="level2">
<h2 class="anchored" data-anchor-id="day-2-recordings-morning-session">â–¶ï¸ Day 2 Recordings: Morning Session</h2>
<section id="talk-tianxi-cai-unlocking-the-potential-of-ehr-data-for-discovery-opportunities-and-challenges" class="level3">
<h3 class="anchored" data-anchor-id="talk-tianxi-cai-unlocking-the-potential-of-ehr-data-for-discovery-opportunities-and-challenges">ğŸ¤ Tianxi Cai: <em>Unlocking the Potential of EHR Data for Discovery: Opportunities and Challenges</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 09:02 - 09:38<br>
ğŸ›ï¸ Harvard University</p>
<strong>Keywords:</strong> longitudinal data, rare disease diagnosis, data heterogeneity<br>

<strong>Summary:</strong> New strategies for leveraging large-scale EHR data are outlined to accelerate precision medicine and translational research by overcoming data heterogeneity and enabling actionable clinical insights.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Electronic Health Record (EHR) data
offers unprecedented opportunities for advancing precision medicine,
enhancing clinical decision support, and accelerating translational
research. By leveraging rich, longitudinal clinical data, we can uncover
patient-specific insights, identify novel risk factors, and tailor
interventions in real time. Large scale EHR data, linked with other data
sources, also open opportunities for drug discovery, repurposing, and
rare disease diagnosis. However, realizing this potential requires
addressing significant challenges, including high dimensionality, data
sparsity, and heterogeneity across healthcare systems. This presentation
will explore strategies for harnessing EHR data effectively, with a
focus on methodological innovations, collaborative infrastructure, and
the importance of clinical context in transforming raw data into
actionable knowledge.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508190902-Cai.mp4" title="BIRS talk: Tianxi Cai" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508190902-Cai.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-xihong-lin-navigate-the-crossroad-of-statistics-generative-ai-and-genomic-health" class="level3">
<h3 class="anchored" data-anchor-id="talk-xihong-lin-navigate-the-crossroad-of-statistics-generative-ai-and-genomic-health">ğŸ¤ Xihong Lin: <em>Navigate the Crossroad of Statistics, Generative AI and Genomic Health</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 09:39 - 10:19<br>
ğŸ›ï¸ Harvard University</p>
<strong>Keywords:</strong> synthetic likelihood, diffusion models, whole-genome sequencing (WGS), biobank data<br>

<strong>Summary:</strong> Generative AI is combined with robust statistical methods to unlock large-scale genomic discovery, enabling valid inference and scalable analysis of biobank and whole-genome sequencing data.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Scalable and robust statistical
methods empowered by generative AI offer unprecedent potentials for
trustworthy science as they empower statistical analysis, quantify
uncertainty, enhance interpretability, and accelerate scientific
discovery. In this talk, I will discuss the challenges and opportunities
as we navigate the crossroad of statistics, generative AI, and genomic
health science. I will discuss robust and powerful statistical analysis
using the synthetic likelihood that leveraging synthetic data generated
by generative AI models, such as diffusion models and transformer, while
ensuring valid statistical inference when generative AI models are
misspecified. I will illustrate key points using the analysis of large
scale biobanks, whole genome sequencing data, and electronic health
records, and demonstrate the power of scientific discovery by
integrating statistics and generative AI using synthetic data. I will
also discuss how to conduct scalable and interpretable large-scale whole
genome sequencing (WGS) data, and illustrate the WGS analysis ecosystem
using the TOPMed WGS samples of 200,000, the UK biobank of 500,000
subjects in the cloud platform RAP and as well the All of Us data of
400,000 subjects in the NIH cloud platform AnVIL.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508190939-Lin.mp4" title="BIRS talk: Xihong Lin" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508190939-Lin.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-annie-qu-representation-retrieval-learning-for-heterogeneous-data-integration" class="level3">
<h3 class="anchored" data-anchor-id="talk-annie-qu-representation-retrieval-learning-for-heterogeneous-data-integration">ğŸ¤ Annie Qu: <em>Representation Retrieval Learning for Heterogeneous Data Integration</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 10:47 - 11:20<br>
ğŸ›ï¸ University of California Irvine</p>
<strong>Keywords:</strong> multi-task learning, optimal transport, latent representation, transferability<br>

<strong>Summary:</strong> A new representation retrieval learning framework improves prediction and inference by borrowing partially shared structures across heterogeneous datasets.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> In this presentation, I will showcase
advanced statistical machine learning techniques and tools designed for
the seamless integration of information from multi-source datasets.
These datasets may originate from various sources, encompass distinct
studies with different variables, and exhibit unique dependent
structures. One of the greatest challenges in investigating research
findings is the systematic heterogeneity across individuals, which could
significantly undermine the power of existing machine learning methods
to identify the underlying true signals. This talk will investigate the
advantages and drawbacks of current data integration methods such as
multi-task learning, optimal transport, missing data imputations, matrix
completions and transfer learning. Additionally, we will introduce a new
representation retriever learning aimed at mapping heterogeneous
observed data to a latent space, facilitating the extraction of shared
information and knowledge, and disentanglement of source-specific
information and knowledge. The key idea is to project heterogeneous raw
observations to representation retriever library, and the novelty of our
method is that we can retrieve partial representations from the library
for a target study. The main advantages of the proposed method are that
it can increase statistical power through borrowing partially shared
representation retrievers from multiple sources of data. This approach
ultimately allows one to extract information from heterogeneous data
sources and transfer generalizable knowledge beyond observed data and
enhance the accuracy of prediction and statistical inference.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191047-Qu.mp4" title="BIRS talk: Annie Qu" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191047-Qu.html">ğŸ¬Open the video directly</a></p>
</section>
</section>
<section id="day-2-recordings-afternoon-session-1" class="level2">
<h2 class="anchored" data-anchor-id="day-2-recordings-afternoon-session-1">â–¶ï¸ Day 2 Recordings: Afternoon Session 1</h2>
<section id="talk-hongzhe-li-decoding-population-diversity-at-single-cell-resolution-with-ai-and-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="talk-hongzhe-li-decoding-population-diversity-at-single-cell-resolution-with-ai-and-machine-learning">ğŸ¤ Hongzhe Li: <em>Decoding Population Diversity at Single-Cell Resolution with AI and Machine Learning</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 13:05 - 13:40<br>
ğŸ›ï¸ University of Pennsylvania</p>
<strong>Keywords:</strong> single-cell omics, trajectory inference, population variability, precision health<br>

<strong>Summary:</strong> AI methods link single-cell profiles with population-level traits, revealing how molecular variation drives human diversity in health and disease.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Understanding population diversity at
the single-cell level is essential for uncovering the biological basis
of development, disease, and therapeutic response. Recent advances in
single-cell technologies enable high-resolution profiling of cellular
states across individuals, but analyzing these large, heterogeneous
datasets remains challenging. AI and machine learning offer powerful
approaches to characterize inter-individual variability, identify cell
types and states, and infer trajectories from complex single-cell data.
This talk will highlight recent methods that integrate single-cell
profiles with individual-level metadataâ€”such as genotype, ancestry,
environmental exposure, and clinical outcomesâ€”to link molecular
variation with population-level traits. These approaches provide new
insights into the cellular basis of human diversity and hold promise for
advancing precision health.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191305-Li.mp4" title="BIRS talk: Hongzhe Li" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191305-Li.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-tianming-liu-quantum-transformers" class="level3">
<h3 class="anchored" data-anchor-id="talk-tianming-liu-quantum-transformers">ğŸ¤ Tianming Liu: <em>Quantum Transformers</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 13:50 - 14:05<br>
ğŸ›ï¸ University of Georgia</p>
<strong>Keywords:</strong> quantum computing, transformer architecture, foundation models<br>

<strong>Summary:</strong> Quantum-based transformer architectures promise to accelerate AI capabilities and broaden applications across science and biomedicine.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Transformers have transformed the AI
field, particularly in recent large language models and other foundation
models. In parallel, quantum computing holds extraordinary promise for
accelerating AI development and application. This talk will introduce a
quantumâ€‘computingâ€‘based implementation of the transformer architecture
and showcase its potential across various applications. Quantum
transformers are envisioned to offer significant benefits in many
domains â€” including biomedicine â€” in the years ahead.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191350-Liu.mp4" title="BIRS talk: Tianming Liu" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191350-Liu.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-tao-wang-profiling-antigen-binding-affinity-of-b-cell-repertoires-in-tumors-by-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="talk-tao-wang-profiling-antigen-binding-affinity-of-b-cell-repertoires-in-tumors-by-deep-learning">ğŸ¤ Tao Wang: <em>Profiling antigen-binding affinity of B cell repertoires in tumors by deep learning</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 14:06 - 14:22<br>
ğŸ›ï¸ MD Anderson Cancer Center</p>
<strong>Keywords:</strong> contrastive learning, antibodyâ€“antigen interactions, immune checkpoint inhibitors, risk scoring<br>

<strong>Summary:</strong> A deep learning framework (Cmai) profiles antigenâ€“antibody binding affinities from BCR repertoires, enabling prediction of immunotherapy response and risk of immune-related adverse events.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> The capability to profile the
landscape of antigen-binding affinities of a vast number of antibodies
(B cell receptors, BCRs) will provide a powerful tool to reveal
biological insights. However, experimental approaches for detecting
antibodyâ€“antigen interactions are costly and time-consuming and can only
achieve low-to-mid throughput. In this work, we developed Cmai
(contrastive modeling for antigenâ€“antibody interactions) to address the
prediction of binding between antibodies and antigens that can be scaled
to high-throughput sequencing data. We devised a biomarker based on the
output from Cmai to map the antigen-binding affinities of BCR
repertoires. We found that the abundance of tumor antigen-targeting
antibodies is predictive of immune-checkpoint inhibitor (ICI) treatment
response. We also found that, during immune-related adverse events
(irAEs) caused by ICI, humoral immunity is preferentially responsive to
intracellular antigens from the organs affected by the irAEs. We used
Cmai to construct a BCR-based irAE risk score, which predicted the
timing of the occurrence of irAEs.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191406-Wang.mp4" title="BIRS talk: Tao Wang" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191406-Wang.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-rong-ma-two-recent-methods-for-nonlinear-joint-embedding-of-high-dimensional-data" class="level3">
<h3 class="anchored" data-anchor-id="talk-rong-ma-two-recent-methods-for-nonlinear-joint-embedding-of-high-dimensional-data">ğŸ¤ Rong Ma: <em>Two recent methods for nonlinear joint embedding of high-dimensional data</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 14:28 - 14:48<br>
ğŸ›ï¸ Harvard University</p>
<strong>Keywords:</strong> optimal transport, integral operators, manifold alignment, multi-omics integration<br>

<strong>Summary:</strong> New methods based on entropic optimal transport and duo-landmark integral operators achieve consistent nonlinear joint embedding, recovering shared manifold structures while removing dataset-specific noise.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> In this talk, I will present two
recent and closely related methods for nonlinear joint embedding of
high-dimensional datasets. The first builds on ideas from entropic
optimal transport, while the second is based on duo-landmark integral
operators. Both are principled approaches for aligning and jointly
embedding multiple datasets, supported by rigorous theoretical
guarantees. We show that for a pair of noisy, high-dimensional datasets,
these methods consistently recover the shared underlying manifold
structure while mitigating dataset-specific nuisance variation. I will
provide an intuitive geometric explanation of each methodology, along
with the theoretical foundations that justify their performance. I will
demonstrate their effectiveness in analyzing a single-cell multiomic
dataset comprising snRNA-seq and snATAC-seq for human brain cells, which
uncovers interesting cell-type-specific interactions between
transcription and epigenomic regulation. This talk is based on recent
work in collaboration with Xiucai Ding, and a work with Boris Landa, and
Yuval Kluger.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191428-Ma.mp4" title="BIRS talk: Rong Ma" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191428-Ma.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-yang-ning-optimal-variable-clustering-for-high-dimensional-matrix-valued-data" class="level3">
<h3 class="anchored" data-anchor-id="talk-yang-ning-optimal-variable-clustering-for-high-dimensional-matrix-valued-data">ğŸ¤ Yang Ning: <em>Optimal Variable Clustering for High-Dimensional Matrix Valued Data</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 14:52 - 15:14<br>
ğŸ›ï¸ Cornell University</p>
<strong>Keywords:</strong> matrix-valued data, latent variable models, covariance clustering, minimax optimality<br>

<strong>Summary:</strong> A latent variable model with covariance-based dissimilarities yields a hierarchical clustering algorithm that is minimax optimal for high-dimensional matrix-valued data and effective in genomic applications.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Matrix valued data has become
increasingly prevalent in many applications. Most of the existing
clustering methods for this type of data are tailored to the mean model
and do not account for the dependence structure of the features, which
can be very informative, especially in high-dimensional settings or when
mean information is not available. To extract the information from the
dependence structure for clustering, we propose a new latent variable
model for the features arranged in matrix form, with some unknown
membership matrices representing the clusters for the rows and columns.
Under this model, we further propose a class of hierarchical clustering
algorithms using the difference of a weighted covariance matrix as the
dissimilarity measure. Theoretically, we show that under mild
conditions, our algorithm attains clustering consistency in the
high-dimensional setting. While this consistency result holds for our
algorithm with a broad class of weighted covariance matrices, the
conditions for this result depend on the choice of the weight. To
investigate how the weight affects the theoretical performance of our
algorithm, we establish the minimax lower bound for clustering under our
latent variable model in terms of some cluster separation metric. Given
these results, we identify the optimal weight in the sense that using
this weight guarantees our algorithm to be minimax rate-optimal. The
practical implementation of our algorithm with the optimal weight is
also discussed. Simulation studies show that our algorithm performs
better than existing methods in terms of the adjusted Rand index (ARI).
The method is applied to a genomic dataset and yields meaningful
interpretations.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191452-Ning.mp4" title="BIRS talk: Yang Ning" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191452-Ning.html">ğŸ¬Open the video directly</a></p>
</section>
</section>
<section id="day-2-recordings-afternoon-session-2" class="level2">
<h2 class="anchored" data-anchor-id="day-2-recordings-afternoon-session-2">â–¶ï¸ Day 2 Recordings: Afternoon Session 2</h2>
<section id="talk-yingqi-zhao-early-classification-of-time-series-with-constraint" class="level3">
<h3 class="anchored" data-anchor-id="talk-yingqi-zhao-early-classification-of-time-series-with-constraint">ğŸ¤ Yingqi Zhao: <em>Early classification of time series with constraint</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 15:32 - 15:55<br>
ğŸ›ï¸ Fred Hutch Cancer Center</p>
<strong>Keywords:</strong> sequential decision making, early classification, trade-off optimization, biomarkers<br>

<strong>Summary:</strong> A constrained early-classification framework for time series balances sensitivity, specificity, and earliness, providing tractable optimal solutions with strong real-world applicability.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Biomarker levels are associated with
adverse events among patients. These adverse events present serious
health risks to affected patients and are associated with significant
financial costs. Thus, a high-quality predictive model that could
identify high-risk patients has the potential to improve patient
outcomes while reducing healthcare costs. From the perspective of
sequential decision making, we propose a novel approach for early
classification of time series incorporating various constraints. The
classifier either concludes positively/negatively based on the series or
waits for further information from the next time step. We characterize
the trade-off among multiple criteria, such as sensitivity, specificity
and earliness. We also explicitly formulate the optimal solution, which
is tractable via plugging-in estimators. Experimental studies
demonstrate its potential in real-world applications.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191532-Zhao.mp4" title="BIRS talk: Yingqi Zhao" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191532-Zhao.html">ğŸ¬Open the video directly</a></p>
</section>
<section id="talk-xiaowu-dai-training-free-multi-agent-language-models" class="level3">
<h3 class="anchored" data-anchor-id="talk-xiaowu-dai-training-free-multi-agent-language-models">ğŸ¤ Xiaowu Dai: <em>Training-Free Multi-Agent Language Models</em></h3>
<p>ğŸ“… Tuesday, August 19, 2025 â€¢ ğŸ•˜ 15:56 - 16:22<br>
ğŸ›ï¸ University of California, Los Angeles</p>
<strong>Keywords:</strong> multi-agent systems, peer elicitation, game theory, truthful equilibrium<br>

<strong>Summary:</strong> A training-free game-theoretic framework (Peer Elicitation Games) aligns LLMs through multi-agent interaction, achieving provably truthful equilibria and improved factual accuracy without supervision.<br>

<details>
  <summary><u>ğŸ“– Read more</u></summary>
  <p><strong>Introduction:</strong> Large Language Models (LLMs) have
demonstrated strong generative capabilities but remain prone to
inconsistencies and hallucinations. We introduce Peer Elicitation Games
(PEG), a training-free, game-theoretic framework for aligning LLMs
through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators
interact in a peer evaluation setting, where rewards are computed using
a determinant-based mutual information score that provably incentivizes
truthful reporting without requiring ground-truth labels. We establish
theoretical guarantees showing that each agent, via online learning,
achieves sublinear regret in the sense their cumulative performance
approaches that of the best fixed truthful strategy in hindsight.
Moreover, we prove last-iterate convergence to a truthful Nash
equilibrium, ensuring that the actual policies used by agents converge
to stable and truthful behavior over time. Empirical evaluations across
multiple benchmarks demonstrate significant improvements in factual
accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or
fine-tuning.</p>
</details>
<div style="position:relative;padding-top:56.25%;height:0;overflow:hidden;margin:1rem 0;">
  <iframe src="https://videos.birs.ca/2025/25w5470/202508191556-Dai.mp4" title="BIRS talk: Xiaowu Dai" allowfullscreen="" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0;">
  </iframe>
</div>
<p><a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos/watch/202508191556-Dai.html">ğŸ¬Open the video directly</a></p>
</section>
</section>
<section id="watch-all-recordings" class="level2">
<h2 class="anchored" data-anchor-id="watch-all-recordings">ğŸ“Œ Watch All Recordings</h2>
<ul>
<li><strong>StatsUpAI YouTube Channel:</strong> <a href="https://www.youtube.com/@StatsUpAI">Subscribe for updates</a></li>
<li><strong>BIRS Official Videos Page:</strong> <a href="https://www.birs.ca/events/2025/5-day-workshops/25w5470/videos">2025 Workshop Videos</a></li>
<li><strong>Direct Video Downloads:</strong> <a href="https://videos.birs.ca/2025/25w5470/">BIRS Video Server</a></li>
</ul>
<!------->
<style>
  .yt-thumb-card{display:flex; align-items:center; justify-content: center; gap:12px;}
  .yt-thumb-card .avatar{width:56px; height:56px; border-radius:50%;}
  .yt-thumb-card .thumb{width:260px; aspect-ratio:16/9; object-fit:cover; border-radius:10px;}
</style>
<!--A minimal thumbnail:-->
<div style="text-align: center; justify-content: center; margin-bottom: 2em;">
<a class="yt-thumb-card" href="https://www.youtube.com/@StatsUpAI" target="_blank"> <img class="avatar" src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj" alt="Stats Up AI" style="width: 20%; height: 20%;"> </a> <a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"> <img src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" alt="Stats Up AI YouTube" width="15%">
<p style="font-size: 14px; margin-top: 0.5em;">
Visit the <strong>Stats Up AI</strong> Channel for More
</p>
</a><p><a href="https://m.youtube.com/@StatsUpAI/videos?view=0&amp;sort=p&amp;shelf_id=3" target="_blank" style="justify-content: center; margin-bottom: 1em;"></a> <!--</div>--></p>
<!--An inline frame (iframe) to embed the YouTube playlist:-->
<!--<div style="text-align: center; margin-top: 1em;">-->
<!--<iframe -->
<!--  max-width="560px"-->
<!--  width="100%"-->
<!--  aspect-ratio="16/9"-->
<!--  src="https://www.youtube.com/embed/videoseries?list=UU2VlJBtA_63CrXP0Sr99P8Q" -->
<!--  title="Uploads playlist"-->
<!--  frameborder="0"-->
<!--  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" -->
<!--  allowfullscreen>-->
<!--</iframe>-->
<!--</div>-->
</div>
<!--YouTube logo:-->
<!--src="https://www.gstatic.com/youtube/img/branding/youtubelogo/svg/youtubelogo.svg" -->
<!--Channel avatar:-->
<!--src="https://yt3.googleusercontent.com/a9XnyV5aES9fTt4MOMrki6BGphDAUIZpDX1-6uebv03GovLZv4nLcyvgeU00atuLYlVuzEAtKcM=s160-c-k-c0x00ffffff-no-rj"-->
<p><em>AI is rapidly reshaping biomedical research by integrating diverse data, accelerating discovery, and supporting decision-making under uncertainty. With statisticians at the forefront, these applications gain the depth, rigor, and reliability needed to truly transform science and medicine.</em></p>


</section>

</main> <!-- /main -->
<!-- GoatCounter Tracker -->
<script data-goatcounter="https://statsupai.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

<!-- Per-Page View Counter (Footer Style) -->
<style>
/*#goatcounter-footer {*/
/*  display: flex; */
/*  justify-content: left;*/
/*  column-gap: 5px;*/
/*  color: #8E8E8E; */
/*  font-size: 13px; */
/*  font-family: Arial, sans-serif;*/
/*}*/
/*#goatcounter-footer .statcounter {*/
/*  font-family: 'Courier New', monospace; */
/*  color: #0033cc; */
/*  font-weight: bold; */
/*  margin-left: 3px; */
/*}*/

#goatcounter-footer {
  display: flex; 
  align-items: center; 
  column-gap: 5px;
  color: #8E8E8E; 
  font-size: 13px; 
  font-family: Arial, sans-serif;
}
#goatcounter-footer .statcounter {
  font-family: 'Courier New', monospace; 
  color: #0033cc; 
  font-weight: bold; 
  margin-left: 3px; 
}
</style>

<!-- GoatCounter Tracker -->
<div id="goatcounter-footer"><span style="margin-left: 3px;">Loading views...</span></div>
<script>
    const path = window.location.pathname;
    console.log('window.location.pathname:', path);
    // If at the root, skip fetching views for now
    if (path.endsWith('index.html') || path.endsWith('/posts/')) {
      
      const display = document.getElementById("goatcounter-footer");
      if (display) {
        display.innerHTML = "";
        console.log("Skipping views for root path:", path);
      }
      
    } else {
      
      // Trim path to remove everything before /quarto_web/site/ for easier local testing
      var trimmedPath = path.replace(/^.*\/posts\//, '\/quarto_web\/site\/posts\/');
      // If the path is exactly /posts/, set trimmedPath to the index.html file
      if (path === '\/posts\/') {
          console.log('Path is exactly /posts/, setting trimmedPath to index.html');
          trimmedPath = '/quarto_web/site/posts/index.html';
      }
      console.log('trimmedPath:', trimmedPath);
      
      // setInterval to check every 100ms because GoatCounter may not be available immediately
      var t = setInterval(function() {
          console.log('Checking for GoatCounter availability...window.goatcounter:', window.goatcounter);
          // Check if GoatCounter is available
          if (window.goatcounter && window.goatcounter.visit_count) {

              // GoatCounter is available, clear the timer
              clearInterval(t)
              // Update the display with the current visit count
              const display = document.getElementById("goatcounter-footer");
              if (display) {
                display.innerHTML = '<span">Page Views: </span>';
                console.log('display.innerHTML:', display.innerHTML);
              }
              
              
              goatcounter.visit_count({
                no_branding: true,
                path: trimmedPath,
                append: '#goatcounter-footer', 
                style: `
                  div { 
                    border-width: 0px;
                    display: flex;
                  }
                  #gcvc-for {
                    font-size: 0px;
                  }
                  #gcvc-views { 
                      font-family: 'Courier New', monospace; 
                      font-size: 13px;
                      color: #0033cc; 
                      font-weight: bold; 
                      margin-top: 29px;
                  }`
              })

          }
      }, 100)
      
    }
</script>






<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>